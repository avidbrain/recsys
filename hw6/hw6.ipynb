{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc4c671",
   "metadata": {},
   "source": [
    "# Рекомендательные системы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeed1de",
   "metadata": {},
   "source": [
    "## Урок 6. Двухуровневые модели рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d19ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import bsr_matrix\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import ItemItemRecommender\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8991c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETAIL_DATA = \"../hw2/retail_train.csv.zip\"\n",
    "PRODUCT_DATA = \"../hw2/product.csv\"\n",
    "DEMOGRAPHIC_DATA = \"../hw5/hh_demographic.csv\"\n",
    "VAL_MATCHER_WEEKS = 6\n",
    "VAL_RANKER_WEEKS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782bb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_ID_COL = 'item_id'\n",
    "USER_ID_COL = 'user_id'\n",
    "ITEM_INDEX_COL = 'item_idx'\n",
    "USER_INDEX_COL = 'user_idx'\n",
    "WEEK_NUM_COL = 'week_no'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcc99d",
   "metadata": {},
   "source": [
    "### Библиотека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300782e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@K\n",
    "def precision_at_k(recommended_list, bought_list, k=5):\n",
    "    try:\n",
    "        _rec_list = recommended_list[:k]\n",
    "        _b_and_r = np.intersect1d(bought_list, _rec_list)\n",
    "        return _b_and_r.size / len(_rec_list)\n",
    "    except (ZeroDivisionError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "def mean_precision_at_k(df, rec, bought, k=5):\n",
    "    _result = df.apply(\n",
    "        lambda row: precision_at_k(row[rec], row[bought], k),\n",
    "        axis=1\n",
    "    )\n",
    "    return np.mean(_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28cb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@K\n",
    "def recall_at_k(recommended_list, bought_list, k=5):\n",
    "    try:\n",
    "        _rec_list = recommended_list[:k]\n",
    "        _b_and_r = np.intersect1d(bought_list, _rec_list)\n",
    "        return _b_and_r.size / len(bought_list)\n",
    "    except (ZeroDivisionError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "def mean_recall_at_k(df, rec, bought, k=5):\n",
    "    _result = df.apply(\n",
    "        lambda row: recall_at_k(row[rec], row[bought], k),\n",
    "        axis=1\n",
    "    )\n",
    "    return np.mean(_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29197b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transform_csv(path, column_map={}, index=None, **kwargs):\n",
    "    columns = pd.read_csv(path, nrows=0).columns\n",
    "    _column_map = dict(zip(columns, columns.str.lower()))\n",
    "    _column_map.update(column_map)\n",
    "    _data = pd.read_csv(path, **kwargs).rename(columns=_column_map)\n",
    "    if index is not None:\n",
    "        return _data.set_index(index)    \n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5ab1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предфильтрация\n",
    "def prefilter_items(data, prevalence_range = (0.05, 0.95), price_range = (1.0, 100.0)):\n",
    "    # Уберем самые популярные товары и самые непопулярные товары\n",
    "    pop_thr, unpop_thr = prevalence_range\n",
    "    item_cum_counts = data[ITEM_ID_COL].value_counts().cumsum()\n",
    "    max_count = item_cum_counts.values[-1]\n",
    "    top_popular_mask = item_cum_counts < max_count * pop_thr\n",
    "    top_uppopular_mask = item_cum_counts > max_count * unpop_thr\n",
    "    blocked_items = item_cum_counts[top_popular_mask | top_uppopular_mask].index\n",
    "    \n",
    "    # Уберем товары, которые не продавались за последние 12 месяцев\n",
    "    recent_sale_items = data[ITEM_ID_COL][data[WEEK_NUM_COL] > data[WEEK_NUM_COL].max() - 53]\n",
    "    old_sale_items = np.setdiff1d(data[ITEM_ID_COL], recent_sale_items)\n",
    "    blocked_items = np.union1d(blocked_items, old_sale_items)\n",
    "    \n",
    "    # Уберем слишком дешевые товары и слишком дорогие товары\n",
    "    # Цена товара косвенно оценивается по sales_value\n",
    "    min_price, max_price = price_range\n",
    "    bad_price_items = (\n",
    "        data\n",
    "        .assign(price = lambda x: np.where(x['quantity'] > 0, x['sales_value'] / x['quantity'], 0.0))\n",
    "        .groupby(ITEM_ID_COL)\n",
    "        .agg(min_item_price=('price', 'min'), max_item_price=('price', 'max'))\n",
    "        .query(\"min_item_price >= @max_price or max_item_price <= @min_price\")\n",
    "        .index\n",
    "    )\n",
    "    blocked_items = np.union1d(blocked_items, bad_price_items)\n",
    "    return data[~np.isin(data[ITEM_ID_COL], blocked_items)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a046755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecStore - класс, аккумулирующий результаты работы разных рекомендательных моделей\n",
    "class RecStore:\n",
    "    _metric_dispatcher = {\n",
    "        'recall': mean_recall_at_k,\n",
    "        'precision': mean_precision_at_k\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data, aggcol, aggfunc):  # training data\n",
    "        agg_df = (\n",
    "            data.groupby([USER_ID_COL, ITEM_ID_COL])\n",
    "            .agg(interaction=(aggcol, aggfunc))\n",
    "            .reset_index()\n",
    "        )\n",
    "        self.user_idx_id = pd.DataFrame(enumerate(np.sort(agg_df[USER_ID_COL].unique())), \n",
    "                                        columns=[USER_INDEX_COL, USER_ID_COL])\n",
    "        self.item_idx_id = pd.DataFrame(enumerate(np.sort(agg_df[ITEM_ID_COL].unique())), \n",
    "                                        columns=[ITEM_INDEX_COL, ITEM_ID_COL])\n",
    "        self.user_item_interaction = (\n",
    "            agg_df\n",
    "            .merge(self.user_idx_id, on=USER_ID_COL)\n",
    "            .merge(self.item_idx_id, on=ITEM_ID_COL)\n",
    "        )\n",
    "        interaction = self.user_item_interaction['interaction'].astype(float)\n",
    "        user_idx = self.user_item_interaction[USER_INDEX_COL]\n",
    "        item_idx = self.user_item_interaction[ITEM_INDEX_COL]\n",
    "        self.user_item_matrix = bsr_matrix((interaction, (user_idx, item_idx)), \n",
    "                                           shape=(user_idx.max()+1, item_idx.max()+1)).tocsr()\n",
    "        self.item_popularity = (\n",
    "            self.user_item_interaction\n",
    "            .groupby(ITEM_ID_COL)\n",
    "            .agg({'interaction': 'sum'})\n",
    "            .sort_values('interaction', ascending=False)\n",
    "            .reset_index()\n",
    "        )\n",
    "        self.recommendations = self.user_idx_id\n",
    "        self.actuals = []\n",
    "    \n",
    "    def add_actual(self, data, name=None):  # test data\n",
    "        if name is None:\n",
    "            name = ACTUAL_COL\n",
    "        self.actuals.append(\n",
    "            data[[USER_ID_COL, ITEM_ID_COL]]\n",
    "            .drop_duplicates()\n",
    "            .groupby(USER_ID_COL)\n",
    "            .agg({ITEM_ID_COL: list})\n",
    "            .rename(columns={ITEM_ID_COL: name})\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def result(self):\n",
    "        _result = self.recommendations\n",
    "        for actual_df in self.actuals:\n",
    "            _result = _result.merge(actual_df, on=USER_ID_COL, how=\"left\")\n",
    "        return _result\n",
    "    \n",
    "    def _get_recommendations(self, model, userid, N):\n",
    "        model_rec = model.recommend(userid, self.user_item_matrix, N=N,\n",
    "                              filter_already_liked_items=False,\n",
    "                              recalculate_user=False)\n",
    "        rec_df = pd.DataFrame(model_rec, columns=[ITEM_INDEX_COL, 'model_score']).set_index(ITEM_INDEX_COL)\n",
    "        rec = rec_df.join(self.item_idx_id)[ITEM_ID_COL]\n",
    "        return rec.tolist()\n",
    "    \n",
    "    def _extend_rec_with_popular(self, lst, N):\n",
    "        if isinstance(lst, list):\n",
    "            if len(lst) == N:\n",
    "                return lst            \n",
    "            else:\n",
    "                series = pd.Series(lst).append(self.item_popularity[ITEM_ID_COL]).unique()[:N]\n",
    "        else:\n",
    "            series = self.item_popularity[ITEM_ID_COL][:N]\n",
    "        return series.tolist()\n",
    "\n",
    "    def add_recommendations(self, model, N=5, model_name=None, extend_with_popular=True, show_progress=False):\n",
    "        if model_name is None:\n",
    "            model_name = f\"{model.__class__.__name__}, N={N}\"\n",
    "        \n",
    "        model.fit(self.user_item_matrix.T, show_progress=show_progress)\n",
    "        if hasattr(model, 'user_factors') and hasattr(model, 'item_factors'):\n",
    "            fast_recs = model.user_factors @ model.item_factors.T\n",
    "            item_ids = self.item_idx_id[ITEM_ID_COL].values\n",
    "            rec_matrix = item_ids[np.argsort(-fast_recs)[:, :N]]\n",
    "            rec_df = pd.DataFrame.from_records(\n",
    "                np.expand_dims(rec_matrix, axis=1),\n",
    "                columns = [model_name],\n",
    "                index=self.user_idx_id[USER_INDEX_COL]\n",
    "            )\n",
    "            self.recommendations = self.recommendations.join(rec_df)\n",
    "        else:        \n",
    "            def _get_user_rec(userid):\n",
    "                return self._get_recommendations(model=model, userid=userid, N=N) \n",
    "        \n",
    "            self.recommendations[model_name] = self.recommendations[USER_INDEX_COL].apply(_get_user_rec)\n",
    "        if extend_with_popular:\n",
    "            def _extend_with_popular(lst):\n",
    "                return self._extend_rec_with_popular(lst, N=N)\n",
    "            \n",
    "            self.recommendations[model_name] = self.recommendations[model_name].apply(_extend_with_popular)\n",
    "        return self\n",
    "    \n",
    "    def rerank(self, model_name, user_item_proba, suffix=', reranked'):\n",
    "        rec_reranked = (\n",
    "            self.recommendations\n",
    "            .filter([USER_ID_COL, model_name])\n",
    "            .rename(columns={model_name: ITEM_ID_COL})\n",
    "            .explode(ITEM_ID_COL)\n",
    "            .assign(orig_rank = lambda x: x.groupby(USER_ID_COL).cumcount())\n",
    "            .merge(user_item_proba, on=[USER_ID_COL, ITEM_ID_COL], how='left')\n",
    "            .assign(sort_factor = lambda x: np.where(x.proba.isna(), x.orig_rank, -x.proba))\n",
    "            .sort_values([USER_ID_COL, 'sort_factor'], ascending=True)\n",
    "            .groupby(USER_ID_COL)\n",
    "            .agg({ITEM_ID_COL: list})\n",
    "            .rename(columns={ITEM_ID_COL: f\"{model_name}{suffix}\"})\n",
    "        )\n",
    "        self.recommendations = self.recommendations.merge(rec_reranked, on=USER_ID_COL, how=\"left\")\n",
    "    \n",
    "    def get_metric_table(self, actual, metrics, dropna=True):\n",
    "        if isinstance(metrics, str):\n",
    "            metrics = [metrics]\n",
    "        metric_holder = {m: [] for m in metrics}\n",
    "        model_names = rs.recommendations.columns[2:].tolist()\n",
    "        _result = rs.result.filter(model_names + [actual])\n",
    "        if dropna:\n",
    "            _result = _result.dropna()\n",
    "        for metric in metrics:\n",
    "            metric_N = metric.lower().split('@')\n",
    "            metric_func_key = metric_N[0]\n",
    "            if metric_func_key not in self._metric_dispatcher:\n",
    "                continue \n",
    "            metric_func = self._metric_dispatcher[metric_func_key]\n",
    "            kwargs = {}\n",
    "            if len(metric_N) > 1 and metric_N[1].isdigit():\n",
    "                kwargs['k'] = int(metric_N[1])\n",
    "            for model_name in model_names:\n",
    "                metric_holder[metric].append(metric_func(_result, model_name, actual, **kwargs))\n",
    "        return pd.DataFrame(metric_holder, index=model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b78288",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "\n",
    "A) Попробуйте различные варианты генерации кандидатов. Какие из них дают наибольший recall@k ?\n",
    "- Пока пробуем отобрать 50 кандидатов (k=50)\n",
    "- Качество измеряем на data_val_matcher: следующие 6 недель после трейна\n",
    "\n",
    "Дают ли own recommendtions + top-popular лучший recall?  \n",
    "\n",
    "B)* Как зависит recall@k от k? Постройте для одной схемы генерации кандидатов эту зависимость для k = {20, 50, 100, 200, 500}  \n",
    "C)* Исходя из прошлого вопроса, как вы думаете, какое значение k является наиболее разумным?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d1c53",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1ba588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased # of items from 83685 to 5426\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "item_features = read_transform_csv(PRODUCT_DATA, {'PRODUCT_ID': ITEM_ID_COL}, index=ITEM_ID_COL, \n",
    "                                   na_values=['Unknown', 'None/Unknown'])\n",
    "user_features = read_transform_csv(DEMOGRAPHIC_DATA, {'household_key': USER_ID_COL}, index=USER_ID_COL,\n",
    "                                  na_values=['Unknown', 'None/Unknown'])\n",
    "# train test split\n",
    "data = pd.read_csv(RETAIL_DATA)\n",
    "# берем данные для тренировки matching модели\n",
    "data_train_matcher = data[data[WEEK_NUM_COL] < data[WEEK_NUM_COL].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)]\n",
    "# берем данные для валидации matching модели\n",
    "data_val_matcher = data[(data[WEEK_NUM_COL] >= data[WEEK_NUM_COL].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)) &\n",
    "                      (data[WEEK_NUM_COL] < data[WEEK_NUM_COL].max() - (VAL_RANKER_WEEKS))]\n",
    "# берем данные для тренировки ranking модели\n",
    "data_train_ranker = data_val_matcher  # Для наглядности. Далее мы добавим изменения, и они будут отличаться\n",
    "# берем данные для теста ranking, matching модели\n",
    "data_val_ranker = data[data[WEEK_NUM_COL] >= data[WEEK_NUM_COL].max() - VAL_RANKER_WEEKS]\n",
    "\n",
    "# Prefiltered\n",
    "data_train_matcher_filtered = prefilter_items(data_train_matcher, prevalence_range = (0.05, 0.75), price_range = (1.8, 50.0))\n",
    "print(f\"Decreased # of items from {data_train_matcher[ITEM_ID_COL].nunique()}\"\n",
    "      f\" to {data_train_matcher_filtered[ITEM_ID_COL].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81db3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Accumulator\n",
    "rs = RecStore(data_train_matcher_filtered, 'quantity', 'count')\n",
    "rs.add_actual(data_val_matcher, 'matcher_actual')\n",
    "rs.add_actual(data_val_ranker, 'ranker_actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63bffaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = [\n",
    "    (\"Own Rec\", ItemItemRecommender(K=1, num_threads=4)),\n",
    "    (\"ALS\", AlternatingLeastSquares(factors=3, regularization=0.05, iterations=10))\n",
    "]\n",
    "\n",
    "for k in (20, 50, 100, 200, 500):\n",
    "    for model_name, model in models:\n",
    "        model_name=f\"{model_name}, N={k}\"\n",
    "        rs.add_recommendations(model, N=k, model_name=model_name, extend_with_popular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c7be47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>recall@100</th>\n",
       "      <th>recall@200</th>\n",
       "      <th>precision@5</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>precision@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=20</th>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.152193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=20</th>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.080495</td>\n",
       "      <td>0.076552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=50</th>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.099907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=50</th>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.080495</td>\n",
       "      <td>0.059907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=100</th>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.099907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=100</th>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.080495</td>\n",
       "      <td>0.059907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=200</th>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>0.122447</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.099907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=200</th>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.103784</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.080495</td>\n",
       "      <td>0.059907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=500</th>\n",
       "      <td>0.053468</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>0.122447</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.099907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=500</th>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.103784</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.080495</td>\n",
       "      <td>0.059907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                recall@20  recall@50  recall@100  recall@200  precision@5  \\\n",
       "Own Rec, N=20    0.053468   0.053468    0.053468    0.053468     0.243584   \n",
       "ALS, N=20        0.026949   0.026949    0.026949    0.026949     0.078488   \n",
       "Own Rec, N=50    0.053468   0.080064    0.080064    0.080064     0.243584   \n",
       "ALS, N=50        0.026949   0.048537    0.048537    0.048537     0.078488   \n",
       "Own Rec, N=100   0.053468   0.080064    0.100154    0.100154     0.243584   \n",
       "ALS, N=100       0.026949   0.048537    0.073420    0.073420     0.078488   \n",
       "Own Rec, N=200   0.053468   0.080064    0.100154    0.122447     0.243584   \n",
       "ALS, N=200       0.026949   0.048537    0.073420    0.103784     0.078488   \n",
       "Own Rec, N=500   0.053468   0.080064    0.100154    0.122447     0.243584   \n",
       "ALS, N=500       0.026949   0.048537    0.073420    0.103784     0.078488   \n",
       "\n",
       "                precision@10  precision@50  \n",
       "Own Rec, N=20       0.195427      0.152193  \n",
       "ALS, N=20           0.080495      0.076552  \n",
       "Own Rec, N=50       0.195427      0.099907  \n",
       "ALS, N=50           0.080495      0.059907  \n",
       "Own Rec, N=100      0.195427      0.099907  \n",
       "ALS, N=100          0.080495      0.059907  \n",
       "Own Rec, N=200      0.195427      0.099907  \n",
       "ALS, N=200          0.080495      0.059907  \n",
       "Own Rec, N=500      0.195427      0.099907  \n",
       "ALS, N=500          0.080495      0.059907  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rs.get_metric_table('matcher_actual', ['recall@20', 'recall@50', 'recall@100', 'recall@200', \n",
    "                                      'precision@5', 'precision@10', 'precision@50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e90f11",
   "metadata": {},
   "source": [
    "Модель \"Own Recommendations + Top Popular\" дает лучше Recall, чем модель ALS. Значения recall растут с ростом k, в какой-то момент перестают расти. Для дальнейшего моделирования можно взять рекомендации модели \"Own Rec, N=200\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222add2",
   "metadata": {},
   "source": [
    "### Второй уровень - Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53100fe",
   "metadata": {},
   "source": [
    "**Задание 2.**\n",
    "\n",
    "Обучите модель 2-ого уровня, при этом:\n",
    "\n",
    "- Добавьте минимум по 2 фичи для юзера, товара и пары юзер-товар\n",
    "\n",
    "- Измерьте отдельно precision@5 модели 1-ого уровня и двухуровневой модели на data_val_ranker\n",
    "\n",
    "- Вырос ли precision@5 при использовании двухуровневой модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4070e85",
   "metadata": {},
   "source": [
    "Нужно обучить модель 2-ого уровня на data_train_ranking, причем только на кандидатах, выбранных во время matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a2d9e",
   "metadata": {},
   "source": [
    "Ниже будет перебор части ранее использованных рекомендательных моделей. Пользователи и их фичи не зависят от выбора модели, поэтому заранее подготовим вспомогательные датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c2d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отмечаем покупки как target=1\n",
    "ranker_ones = (\n",
    "    data_train_ranker[[USER_ID_COL, ITEM_ID_COL]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([USER_ID_COL, ITEM_ID_COL])\n",
    "    .assign(target=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f674d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User features\n",
    "user_feature_base = (\n",
    "    data_train_ranker\n",
    "    .merge(item_features, on=ITEM_ID_COL, how='left')\n",
    "    .merge(user_features, on=USER_ID_COL, how='left')\n",
    ")\n",
    "\n",
    "median_basket_value = (\n",
    "    user_feature_base\n",
    "    .groupby([USER_ID_COL, 'basket_id'])\n",
    "    .agg({'sales_value': 'sum'})\n",
    "    .groupby(level=0)\n",
    "    .agg(median_basket_value=('sales_value', 'median'))\n",
    ")\n",
    "\n",
    "median_categories_in_basket = (\n",
    "    user_feature_base\n",
    "    .groupby([USER_ID_COL, 'basket_id'])\n",
    "    .agg({'commodity_desc': 'nunique'})\n",
    "    .groupby(level=0)\n",
    "    .agg(median_categories_in_basket=('commodity_desc', 'median'))\n",
    ")\n",
    "\n",
    "week_variance = (\n",
    "    user_feature_base\n",
    "    .groupby([USER_ID_COL, WEEK_NUM_COL])\n",
    "    .agg({'quantity': 'sum'})\n",
    "    .groupby(level=0)\n",
    "    .agg(week_variance=('quantity', 'var'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7590df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Кандидаты - результат работы одной из моделей. Попробуем разные модели\n",
    "for reranking_model_name in ('ALS, N=50', 'ALS, N=200', 'Own Rec, N=200'):\n",
    "    candidates = rs.result[[USER_ID_COL, reranking_model_name]]\n",
    "    df_ranker_train = (\n",
    "        ranker_ones[[USER_ID_COL]]\n",
    "        .drop_duplicates()  # only ranker users\n",
    "        .merge(candidates, on=USER_ID_COL, how='inner')\n",
    "        .rename(columns={reranking_model_name: ITEM_ID_COL})\n",
    "        .explode(ITEM_ID_COL)\n",
    "        .merge(ranker_ones, on=[USER_ID_COL, ITEM_ID_COL], how=\"left\")\n",
    "        .fillna(0)\n",
    "        .merge(item_features, on=ITEM_ID_COL, how='left')\n",
    "        .merge(user_features, on=USER_ID_COL, how='left')\n",
    "    )\n",
    "    cat_features = [col for col in df_ranker_train.columns\n",
    "                    if col not in (USER_ID_COL, ITEM_ID_COL, 'target')]\n",
    "    # Additional feature generation\n",
    "    # Item features\n",
    "    present_items = user_feature_base[ITEM_ID_COL].unique()\n",
    "    absent_items = np.setdiff1d(df_ranker_train[ITEM_ID_COL], present_items)\n",
    "    additional_base = (\n",
    "        data_train_matcher[data_train_matcher[ITEM_ID_COL].isin(absent_items)]\n",
    "        .merge(item_features, on=ITEM_ID_COL, how='left')\n",
    "        .merge(user_features, on=USER_ID_COL, how='left')\n",
    "    )\n",
    "    item_feature_base = pd.concat([user_feature_base, additional_base])\n",
    "\n",
    "    median_num_per_week = (\n",
    "        item_feature_base\n",
    "        .groupby([ITEM_ID_COL, WEEK_NUM_COL])\n",
    "        .agg({'quantity': 'sum'})\n",
    "        .groupby(level=0)\n",
    "        .agg(median_num_per_week=('quantity', 'median'))\n",
    "    )\n",
    "\n",
    "    average_price = (\n",
    "        item_feature_base\n",
    "        .assign(price = lambda x: np.where(x.quantity > 0, x.sales_value/x.quantity, np.nan))\n",
    "        .groupby(ITEM_ID_COL)\n",
    "        .agg(average_price=('price', np.nanmean))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    df_ranker_train_enriched = (\n",
    "        df_ranker_train\n",
    "        .merge(median_basket_value, on=USER_ID_COL, how=\"left\")\n",
    "        .merge(median_categories_in_basket, on=USER_ID_COL, how=\"left\")\n",
    "        .merge(week_variance, on=USER_ID_COL, how=\"left\")\n",
    "        .merge(median_num_per_week, on=ITEM_ID_COL, how=\"left\")\n",
    "        .merge(average_price, on=ITEM_ID_COL, how=\"left\")\n",
    "    )\n",
    "    \n",
    "    X_train = df_ranker_train_enriched.drop(columns=['target', USER_ID_COL, ITEM_ID_COL])\n",
    "    X_train[cat_features] = X_train[cat_features].astype('category')\n",
    "    y_train = df_ranker_train_enriched['target']\n",
    "    lgb = LGBMClassifier(objective='binary',\n",
    "                         max_depth=8,\n",
    "                         n_estimators=300,\n",
    "                         learning_rate=0.05).fit(X_train, y_train)\n",
    "    user_item_proba = (\n",
    "        df_ranker_train_enriched\n",
    "        .assign(proba=lgb.predict_proba(X_train)[:,1])\n",
    "        .filter([USER_ID_COL, ITEM_ID_COL, 'proba'])\n",
    "    )\n",
    "    rs.rerank(reranking_model_name, user_item_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3df4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.72 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall@50</th>\n",
       "      <th>precision@5</th>\n",
       "      <th>precision@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=20</th>\n",
       "      <td>0.054942</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=50</th>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=100</th>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=200</th>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=500</th>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.212783</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own Rec, N=200, reranked</th>\n",
       "      <td>0.063905</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.095428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=50, reranked</th>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.108161</td>\n",
       "      <td>0.093559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=200, reranked</th>\n",
       "      <td>0.054491</td>\n",
       "      <td>0.104228</td>\n",
       "      <td>0.091101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=20</th>\n",
       "      <td>0.028206</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=50</th>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=100</th>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=200</th>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS, N=500</th>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          recall@50  precision@5  precision@10\n",
       "Own Rec, N=20              0.054942     0.212783      0.166667\n",
       "Own Rec, N=50              0.082250     0.212783      0.166667\n",
       "Own Rec, N=100             0.082250     0.212783      0.166667\n",
       "Own Rec, N=200             0.082250     0.212783      0.166667\n",
       "Own Rec, N=500             0.082250     0.212783      0.166667\n",
       "Own Rec, N=200, reranked   0.063905     0.112291      0.095428\n",
       "ALS, N=50, reranked        0.052070     0.108161      0.093559\n",
       "ALS, N=200, reranked       0.054491     0.104228      0.091101\n",
       "ALS, N=20                  0.028206     0.073255      0.065388\n",
       "ALS, N=50                  0.052070     0.073255      0.065388\n",
       "ALS, N=100                 0.052070     0.073255      0.065388\n",
       "ALS, N=200                 0.052070     0.073255      0.065388\n",
       "ALS, N=500                 0.052070     0.073255      0.065388"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rs.get_metric_table('ranker_actual', ['recall@50', 'precision@5', 'precision@10']).sort_values('precision@5', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a9202",
   "metadata": {},
   "source": [
    "Метрика precision@5 при использовании двухуровневой модели и Own Recommendations, N=200 в качестве основной - не выросла. Должна ли она обязательно вырастать? Для ALS-моделей рост при переранжировании заметен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7a667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
