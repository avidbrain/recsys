{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендательные системы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Курсовой проект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: precision@5 от 0.2520 до 0.2546 в разные запуски (фактор рандомности, видимо)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечания**\n",
    "\n",
    "- в домашних работах частично переписывал код методичек для лучшего усвоения материала, в курсовом старался вносить минимальные изменения в предложенный код\n",
    "- тем не менее, убрал иллюстративные и отладочные фрагменты, немного перекомпоновал блоки\n",
    "- в целом данное начальное знакомство с направлением рекомендательных систем оставило впечатление определенного шаманства - много методов, все логичные, некоторые - с хорошим математическим обоснованием, но предсказать, что именно и как сработает без опыта практически невозможно "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внесенные изменения:\n",
    "\n",
    "- N_PREDICT = 9\n",
    "- Заменил prefilter_items на собственную версию, число items получилось в районе 4000, а не 5000\n",
    "- Чуть подкрутил BM25 в MainRecommender\n",
    "- Убрал user_id и item_id из модели второго уровня (кажется, это немного снизило результат, но ID не принято отдавать в бустинг)\n",
    "- Раскомментировал все предложенные фичи (прибавка есть, хотя и весьма небольшая)\n",
    "- Добавил значение, которое выдает ALS, в качестве фичи\n",
    "- Выставил learning_rate=0.07\n",
    "- Убрал раздел с result_eval_ranker для увеличения скорости\n",
    "- Переделал фунцию rerank так, чтобы в reranked_own_rec не было пустых рекомендаций (кстати, это снизило precision@5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit import als\n",
    "\n",
    "# Модель второго уровня\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Написанные нами функции\n",
    "from metrics import precision_at_k, recall_at_k\n",
    "\n",
    "# С правками для курсового:\n",
    "from student_utils import prefilter_items\n",
    "from student_recommenders import MainRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(df_data, top_k):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: recall_at_k(row[col_name], row[ACTUAL_COL], k=top_k), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(df_data, top_k):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: precision_at_k(row[col_name], row[ACTUAL_COL], k=top_k), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_COL = 'item_id'\n",
    "USER_COL = 'user_id'\n",
    "ACTUAL_COL = 'actual'\n",
    "\n",
    "N_PREDICT = 9\n",
    "\n",
    "TOPK_RECALL = 50\n",
    "TOPK_PRECISION = 5\n",
    "\n",
    "VAL_MATCHER_WEEKS = 6\n",
    "VAL_RANKER_WEEKS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../hw2/retail_train.csv.zip')\n",
    "item_features = pd.read_csv('../hw2/product.csv')\n",
    "user_features = pd.read_csv('../hw5/hh_demographic.csv')\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "item_features.rename(columns={'product_id': ITEM_COL}, inplace=True)\n",
    "user_features.rename(columns={'household_key': USER_COL }, inplace=True)\n",
    "df_test = pd.read_csv('retail_test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset for train, eval, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# берем данные для тренировки matching модели\n",
    "data_train_matcher = data[data['week_no'] < data['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)]\n",
    "\n",
    "# берем данные для валидации matching модели\n",
    "data_val_matcher = data[(data['week_no'] >= data['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)) &\n",
    "                      (data['week_no'] < data['week_no'].max() - (VAL_RANKER_WEEKS))]\n",
    "\n",
    "# берем данные для тренировки ranking модели\n",
    "data_train_ranker = data_val_matcher.copy()  # Для наглядности. Далее мы добавим изменения, и они будут отличаться\n",
    "\n",
    "# берем данные для теста ranking, matching модели\n",
    "data_val_ranker = data[data['week_no'] >= data['week_no'].max() - VAL_RANKER_WEEKS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем объединенный сет данных для первого уровня (матчинга)\n",
    "df_join_train_matcher = pd.concat([data_train_matcher, data_val_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefilter items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased # items from 83685 to 3921\n"
     ]
    }
   ],
   "source": [
    "n_items_before = data_train_matcher['item_id'].nunique()\n",
    "\n",
    "#data_train_matcher = prefilter_items(data_train_matcher, item_features=item_features, take_n_popular=5000)\n",
    "data_train_matcher = prefilter_items(data_train_matcher, prevalence_range = (0.09, 0.61), price_range = (0.5, 50.0))\n",
    "\n",
    "n_items_after = data_train_matcher['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make cold-start to warm-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем общих пользователей\n",
    "common_users = data_train_matcher.user_id.values\n",
    "\n",
    "data_val_matcher = data_val_matcher[data_val_matcher.user_id.isin(common_users)]\n",
    "data_train_ranker = data_train_ranker[data_train_ranker.user_id.isin(common_users)]\n",
    "data_val_ranker = data_val_ranker[data_val_ranker.user_id.isin(common_users)]\n",
    "df_test = df_test[df_test.user_id.isin(common_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init/train recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14520bc17a384f78aee1dd9e9e7b237d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec700b9f4024896b182cc2742b2a505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommender = MainRecommender(data_train_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval recall of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[853529, 865456, 867607, 872137, 874905, 87524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[15830248, 838136, 839656, 861272, 866211, 870...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [853529, 865456, 867607, 872137, 874905, 87524...\n",
       "1        2  [15830248, 838136, 839656, 861272, 866211, 870..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_eval_matcher = data_val_matcher.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "result_eval_matcher.columns=[USER_COL, ACTUAL_COL]\n",
    "result_eval_matcher.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_eval_matcher['own_rec'] = result_eval_matcher[USER_COL].apply(\n",
    "    lambda x: recommender.get_own_recommendations(x, N=N_PREDICT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall@50 of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('own_rec', 0.0486123934714024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(calc_recall(result_eval_matcher, TOPK_RECALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@5 of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('own_rec', 0.3211333023687878)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(calc_precision(result_eval_matcher, TOPK_PRECISION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# взяли пользователей из трейна для ранжирования\n",
    "df_match_candidates = pd.DataFrame(data_train_ranker[USER_COL].unique())\n",
    "df_match_candidates.columns = [USER_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем кандитатов с первого этапа (matcher)\n",
    "df_match_candidates['candidates'] = df_match_candidates[USER_COL].apply(\n",
    "    lambda x: recommender.get_own_recommendations(x, N=N_PREDICT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разворачиваем товары\n",
    "df_items = df_match_candidates.apply(lambda x: pd.Series(x['candidates']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "df_items.name = 'item_id'\n",
    "df_match_candidates = df_match_candidates.drop('candidates', axis=1).join(df_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем трейн сет для ранжирования с учетом кандидатов с этапа 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train = data_train_ranker[[USER_COL, ITEM_COL]].copy()\n",
    "df_ranker_train['target'] = 1  # тут только покупки \n",
    "\n",
    "df_ranker_train = df_match_candidates.merge(df_ranker_train, on=[USER_COL, ITEM_COL], how='left')\n",
    "\n",
    "# чистим дубликаты\n",
    "df_ranker_train = df_ranker_train.drop_duplicates(subset=[USER_COL, ITEM_COL])\n",
    "\n",
    "df_ranker_train['target'].fillna(0, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготавливаем фичи для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train = df_ranker_train.merge(item_features, on=ITEM_COL, how='left')\n",
    "df_ranker_train = df_ranker_train.merge(user_features, on=USER_COL, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = df_ranker_train.columns[3:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS feature\n",
    "fast_recs = recommender.model.user_factors @ recommender.model.item_factors.T\n",
    "df_ranker_train['als_score'] = df_ranker_train.apply(\n",
    "    lambda x: fast_recs[recommender.userid_to_id[x.user_id], \n",
    "                        recommender.itemid_to_id[x.item_id]], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поведенческие фичи\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg('sales_value').sum().rename('total_item_sales_value'), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg('quantity').sum().rename('total_quantity_value'), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg(USER_COL).count().rename('item_freq'), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=USER_COL).agg(USER_COL).count().rename('user_freq'), how='left',on=USER_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=USER_COL).agg('sales_value').sum().rename('total_user_sales_value'), how='left',on=USER_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg('quantity').sum().rename('item_quantity_per_week')/df_join_train_matcher.week_no.nunique(), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=USER_COL).agg('quantity').sum().rename('user_quantity_per_week')/df_join_train_matcher.week_no.nunique(), how='left',on=USER_COL)\n",
    "\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg('quantity').sum().rename('item_quantity_per_basket')/df_join_train_matcher.basket_id.nunique(), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=USER_COL).agg('quantity').sum().rename('user_quantity_per_baskter')/df_join_train_matcher.basket_id.nunique(), how='left',on=USER_COL)\n",
    "\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=ITEM_COL).agg(USER_COL).count().rename('item_freq_per_basket')/df_join_train_matcher.basket_id.nunique(), how='left',on=ITEM_COL)\n",
    "\n",
    "df_ranker_train = df_ranker_train.merge(df_join_train_matcher.groupby(by=USER_COL).agg(USER_COL).count().rename('user_freq_per_basket')/df_join_train_matcher.basket_id.nunique(), how='left',on=USER_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_ranker_train.drop(['target', USER_COL, ITEM_COL], axis=1)\n",
    "y_train = df_ranker_train['target']\n",
    "X_train[cat_feats] = X_train[cat_feats].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели ранжирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(objective='binary',\n",
    "                     max_depth=10,\n",
    "                     n_estimators=500,\n",
    "                     learning_rate=0.07)\n",
    "\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "train_preds = lgb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker_predict = (\n",
    "    df_ranker_train\n",
    "    .filter([USER_COL, ITEM_COL])\n",
    "    .assign(proba=train_preds[:,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка на тесте для выполнения курсового проекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_test = df_test.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "result_test.columns=[USER_COL, ACTUAL_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем топ-k предсказаний, ранжированных по вероятности, для каждого юзера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranked(recommendations, user_item_proba, prefix = 'reranked_'):\n",
    "    rec_col = recommendations.columns[-1]\n",
    "    proba_col = user_item_proba.columns[-1]\n",
    "    _uip = user_item_proba.filter([USER_COL, ITEM_COL, proba_col])\n",
    "\n",
    "    return (\n",
    "        recommendations\n",
    "        .rename(columns={rec_col:ITEM_COL})\n",
    "        .explode(ITEM_COL)\n",
    "        .assign(orig_rank = lambda x: x.groupby(level=-1).cumcount())\n",
    "        .merge(_uip, on=[USER_COL, ITEM_COL], how='left')\n",
    "        .assign(sort_factor = lambda x: np.where(x[proba_col].isna(), x.orig_rank, -x[proba_col]))\n",
    "        .sort_values([USER_COL, 'sort_factor'], ascending=True)\n",
    "        .groupby(USER_COL)\n",
    "        .agg({ITEM_COL: list})\n",
    "        .rename(columns={ITEM_COL:f\"{prefix}{rec_col}\"})\n",
    "        .reset_index()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_test['own_rec'] = result_test[USER_COL].apply(lambda x: recommender.get_own_recommendations(x, N=N_PREDICT))\n",
    "#result_test['reranked_own_rec'] = result_test[USER_COL].apply(lambda user_id: rerank(user_id))\n",
    "result_test = result_test.merge(reranked(result_test, df_ranker_predict), on=USER_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reranked_own_rec', 0.2546709129511677)\n",
      "('own_rec', 0.23365180467091298)\n"
     ]
    }
   ],
   "source": [
    "print(*sorted(calc_precision(result_test, TOPK_PRECISION), key=lambda x: x[1], reverse=True), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
